{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Policy Gradient Demonstration\\n\",\n",
    "    \"This notebook provides a detailed walkthrough of our Policy Gradient implementation.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Import required libraries\\n\",\n",
    "    \"import gymnasium as gym\\n\",\n",
    "    \"import numpy as np\\n\",\n",
    "    \"import torch\\n\",\n",
    "    \"import torch.nn as nn\\n\",\n",
    "    \"import torch.optim as optim\\n\",\n",
    "    \"import matplotlib.pyplot as plt\\n\",\n",
    "    \"from agents.policy_agent import PolicyAgent\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Initialize environment and agent\\n\",\n",
    "    \"env = gym.make(\\\"CartPole-v1\\\")\\n\",\n",
    "    \"state_size = env.observation_space.shape[0]\\n\",\n",
    "    \"action_size = env.action_space.n\\n\",\n",
    "    \"agent = PolicyAgent(state_size, action_size)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Training parameters\\n\",\n",
    "    \"num_episodes = 1000\\n\",\n",
    "    \"gamma = 0.99\\n\",\n",
    "    \"learning_rate = 0.01\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Training loop\\n\",\n",
    "    \"rewards = []\\n\",\n",
    "    \"optimizer = optim.Adam(agent.policy_network.parameters(), lr=learning_rate)\\n\",\n",
    "    \"\\n\",\n",
    "    \"for episode in range(num_episodes):\\n\",\n",
    "    \"    state, _ = env.reset()\\n\",\n",
    "    \"    log_probs = []\\n\",\n",
    "    \"    rewards_episode = []\\n\",\n",
    "    \"    done = False\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    while not done:\\n\",\n",
    "    \"        state_tensor = torch.FloatTensor(state).unsqueeze(0)\\n\",\n",
    "    \"        action, log_prob = agent.act(state_tensor)\\n\",\n",
    "    \"        next_state, reward, terminated, truncated, _ = env.step(action)\\n\",\n",
    "    \"        done = terminated or truncated\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        log_probs.append(log_prob)\\n\",\n",
    "    \"        rewards_episode.append(reward)\\n\",\n",
    "    \"        state = next_state\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Calculate discounted rewards\\n\",\n",
    "    \"    discounted_rewards = []\\n\",\n",
    "    \"    cumulative_reward = 0\\n\",\n",
    "    \"    for r in reversed(rewards_episode):\\n\",\n",
    "    \"        cumulative_reward = r + gamma * cumulative_reward\\n\",\n",
    "    \"        discounted_rewards.insert(0, cumulative_reward)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Normalize discounted rewards\\n\",\n",
    "    \"    discounted_rewards = torch.FloatTensor(discounted_rewards)\\n\",\n",
    "    \"    discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e-9)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Calculate policy loss\\n\",\n",
    "    \"    policy_loss = []\\n\",\n",
    "    \"    for log_prob, reward in zip(log_probs, discounted_rewards):\\n\",\n",
    "    \"        policy_loss.append(-log_prob * reward)\\n\",\n",
    "    \"    policy_loss = torch.cat(policy_loss).sum()\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Update policy\\n\",\n",
    "    \"    optimizer.zero_grad()\\n\",\n",
    "    \"    policy_loss.backward()\\n\",\n",
    "    \"    optimizer.step()\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    total_reward = sum(rewards_episode)\\n\",\n",
    "    \"    rewards.append(total_reward)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    if episode % 10 == 0:\\n\",\n",
    "    \"        print(f\\\"Episode: {episode + 1}, Total Reward: {total_reward}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"env.close()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Plot training results\\n\",\n",
    "    \"plt.figure(figsize=(10, 6))\\n\",\n",
    "    \"plt.plot(rewards)\\n\",\n",
    "    \"plt.title(\\\"Policy Gradient Training Progress\\\")\\n\",\n",
    "    \"plt.xlabel(\\\"Episode\\\")\\n\",\n",
    "    \"plt.ylabel(\\\"Total Reward\\\")\\n\",\n",
    "    \"plt.grid(True)\\n\",\n",
    "    \"plt.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Save trained model\\n\",\n",
    "    \"torch.save(agent.policy_network.state_dict(), \\\"policy_gradient_model.pth\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Evaluate the trained agent\\n\",\n",
    "    \"agent.policy_network.load_state_dict(torch.load(\\\"policy_gradient_model.pth\\\"))\\n\",\n",
    "    \"evaluate(env_name=\\\"CartPole-v1\\\", model_path=\\\"policy_gradient_model.pth\\\", num_episodes=100)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Visualize the agent's performance\\n\",\n",
    "    \"visualize(env_name=\\\"CartPole-v1\\\", model_path=\\\"policy_gradient_model.pth\\\", num_episodes=5)\"\n",
    "   ]\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"Python 3\",\n",
    "   \"language\": \"python\",\n",
    "   \"name\": \"python3\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"codemirror_mode\": {\n",
    "    \"name\": \"ipython\",\n",
    "    \"version\": 3\n",
    "   },\n",
    "   \"file_extension\": \".py\",\n",
    "   \"mimetype\": \"text/x-python\",\n",
    "   \"name\": \"python\",\n",
    "   \"nbconvert_exporter\": \"python\",\n",
    "   \"pygments_lexer\": \"ipython3\",\n",
    "   \"version\": \"3.9.0\"\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 4\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
