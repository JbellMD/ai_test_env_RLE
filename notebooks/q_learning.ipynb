{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Q-Learning Demonstration\\n\",\n",
    "    \"This notebook provides a detailed walkthrough of our Q-Learning implementation.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Import required libraries\\n\",\n",
    "    \"import gymnasium as gym\\n\",\n",
    "    \"import numpy as np\\n\",\n",
    "    \"import torch\\n\",\n",
    "    \"import matplotlib.pyplot as plt\\n\",\n",
    "    \"from agents.q_agent import QAgent\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Initialize environment and agent\\n\",\n",
    "    \"env = gym.make(\\\"CartPole-v1\\\")\\n\",\n",
    "    \"state_size = env.observation_space.shape[0]\\n\",\n",
    "    \"action_size = env.action_space.n\\n\",\n",
    "    \"agent = QAgent(state_size, action_size)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Training parameters\\n\",\n",
    "    \"num_episodes = 1000\\n\",\n",
    "    \"batch_size = 64\\n\",\n",
    "    \"gamma = 0.99\\n\",\n",
    "    \"epsilon_start = 1.0\\n\",\n",
    "    \"epsilon_end = 0.01\\n\",\n",
    "    \"epsilon_decay = 0.995\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Training loop\\n\",\n",
    "    \"rewards = []\\n\",\n",
    "    \"epsilon = epsilon_start\\n\",\n",
    "    \"\\n\",\n",
    "    \"for episode in range(num_episodes):\\n\",\n",
    "    \"    state, _ = env.reset()\\n\",\n",
    "    \"    total_reward = 0\\n\",\n",
    "    \"    done = False\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    while not done:\\n\",\n",
    "    \"        action = agent.act(state, epsilon)\\n\",\n",
    "    \"        next_state, reward, terminated, truncated, _ = env.step(action)\\n\",\n",
    "    \"        done = terminated or truncated\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        agent.remember(state, action, reward, next_state, done)\\n\",\n",
    "    \"        agent.train(batch_size, gamma)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        state = next_state\\n\",\n",
    "    \"        total_reward += reward\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    rewards.append(total_reward)\\n\",\n",
    "    \"    epsilon = max(epsilon_end, epsilon * epsilon_decay)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    if episode % 10 == 0:\\n\",\n",
    "    \"        print(f\\\"Episode: {episode + 1}, Total Reward: {total_reward}, Epsilon: {epsilon:.2f}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"env.close()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Plot training results\\n\",\n",
    "    \"plt.figure(figsize=(10, 6))\\n\",\n",
    "    \"plt.plot(rewards)\\n\",\n",
    "    \"plt.title(\\\"Q-Learning Training Progress\\\")\\n\",\n",
    "    \"plt.xlabel(\\\"Episode\\\")\\n\",\n",
    "    \"plt.ylabel(\\\"Total Reward\\\")\\n\",\n",
    "    \"plt.grid(True)\\n\",\n",
    "    \"plt.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Save trained model\\n\",\n",
    "    \"torch.save(agent.q_network.state_dict(), \\\"q_learning_model.pth\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Evaluate the trained agent\\n\",\n",
    "    \"agent.q_network.load_state_dict(torch.load(\\\"q_learning_model.pth\\\"))\\n\",\n",
    "    \"evaluate(env_name=\\\"CartPole-v1\\\", model_path=\\\"q_learning_model.pth\\\", num_episodes=100)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Visualize the agent's performance\\n\",\n",
    "    \"visualize(env_name=\\\"CartPole-v1\\\", model_path=\\\"q_learning_model.pth\\\", num_episodes=5)\"\n",
    "   ]\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"Python 3\",\n",
    "   \"language\": \"python\",\n",
    "   \"name\": \"python3\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"codemirror_mode\": {\n",
    "    \"name\": \"ipython\",\n",
    "    \"version\": 3\n",
    "   },\n",
    "   \"file_extension\": \".py\",\n",
    "   \"mimetype\": \"text/x-python\",\n",
    "   \"name\": \"python\",\n",
    "   \"nbconvert_exporter\": \"python\",\n",
    "   \"pygments_lexer\": \"ipython3\",\n",
    "   \"version\": \"3.9.0\"\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 4\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
